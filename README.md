# PhD Vulnerability Detection Prerequisites

## TODO
* Identify failure conditions for successful training. They should be fully learnable problems as there is zero noise.
* Try training CodeBERT with no frozen weights (running on fry now)
* Simplify code


## Results

Metric is binary accuracy, all datasets are balanced.

| # | Model       | Task 1    | Task 2     | Task 3    | Task 4     | Task 5    |
|---|-------------|-----------|------------|-----------|------------|-----------|
| 1 | Simple MLP  | ðŸŸ¢ 0.9967 | ðŸŸ¢ 0.9955 | ðŸ”´ 0.5068 | ðŸ”´ 0.5080 | ðŸŸ¡ 0.5375 |
| 2 | CNN + MLP   | ðŸŸ¢ 0.9970 | ðŸ”´ 0.5052 | ðŸ”´ 0.5068 | ðŸ”´ 0.5080 | ðŸ”´ 0.5027 |
| 3 | LSTM + MLP  | ðŸŸ¢ 0.9936 | ðŸ”´ 0.5052 | ðŸ”´ 0.5068 | ðŸ”´ 0.5080 | ðŸ”´ 0.5027 |
| 4 | CodeBERT    | âšª 0.5052 | âšª 0.5054 | âšª 0.5067 | âšª 0.xxxx | âšª 0.xxxx |
| 5 | GNN         | ðŸŸ¢ 0.9288 | ðŸŸ¡ 0.7259 | ðŸ”´ 0.5038 | ðŸ”´ 0.5026 | ðŸŸ¡ 0.5365 |

### Legend
* ðŸŸ¢ >= 0.90
* ðŸŸ¡ ~> 0.50
* ðŸ”´ ~= 0.50 (Due to size of sample, may not be exactly 0.5000)
* âšª Untested


### Training Info
#### Simple MLP
| Property         | Value               |
|------------------|---------------------|
| Optimizer        | Adam                |
| Loss             | Binary Crossentropy |
| Epochs           | 10                  |
| Steps per Epoch  | 10,000              |
| Validation Steps | 1,000               |

1. Simple MLP
2. CNN + MLP (commonly used in MLAVD)
3. RNN (probably an LSTM for efficiency)
4. Transformer (BERT-style)
5. GNN


## Notes
* The modulo operator for python and C++ work differently for negative numbers. Because the exact operation of the modulo isn't issue, but being able to evaluate it I've decided to ignore the difference.
* Not Supported
    * Assignment Operators (+=, -=, etc.)
    * Increment/decrement (++, --)


